{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Workshop Developing AI Agents with AMD GPUs: Build Your OpenManus Agent with MCPs using vLLM, and AMD Instinct MI300X GPU\n",
        "\n",
        "Welcome to this hands-on workshop! Throughout this tutorial, we'll leverage AMD GPUs and **Model Context Protocol (MCP)** ,an open standard for exposing LLM tools via API, to deploy powerful language models like Qwen3. Key components:\n",
        "- üñ•Ô∏è **vLLM** for GPU-optimized inference\n",
        "- üõ†Ô∏è **OpenManus** for agent/tool management\n",
        "- üîå **MCP Servers** for pre-built tool integration\n",
        "\n",
        "You'll learn how to set up your environment, deploy large language models like Qwen3, connect them to real-world tools using MCP, and build a conversational agent capable of reasoning and taking actions.\n",
        "\n",
        "By the end of this workshop, you‚Äôll have built an AI-powered Airbnb assistant agent‚Äîone that can find a place to stay based on your preferences like location, budget, and travel dates.\n",
        "\n",
        "Let‚Äôs dive in!\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Step 1: Launching vLLM Server on AMD GPUs](#step1)\n",
        "- [Step 2: Installing Dependencies](#step2)\n",
        "- [Step 3: Create a simple instance of OpenManus](#step3)\n",
        "- [Step 4: Tsing MCP server for OpenManus Agent](#step4)\n",
        "- [Step 5: Challenge with Prize](#step5)\n",
        "\n",
        "<a id=\"step1\"></a>\n",
        "\n",
        "## Step 1: Launch a vLLM Server\n",
        "\n",
        "In this workshop, we are going to use [vLLM](https://github.com/vllm-project/vllm) as our inference serving engine. vLLM provides many benefits such as fast model execution, extensive list of supported models, is easy to use, and best of all, it's open-source. \n",
        "\n",
        "### Deploy Qwen3-30B-A3B Model with vLLM\n",
        "\n",
        "Time to start your vLLM server and create an end-point for your LLM. Let's open a terminal using your Jupyter server and configure your view. \n",
        " \n",
        "> Open a new Jupyter tab by clicking on `+`.<br>\n",
        "> Find the **terminal icon** in your Jupyter environment (not a notebook cell).<br>\n",
        "> **paste this command to launch your vLLM server:**\n",
        "\n",
        "![llm_metrics](./assets/ws201_3.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Once your terminal is open and you configured the screen the way you want, we can move on to starting our vLLM server to create an open-ai compatible end-point model served for our AI agent. \n",
        "\n",
        " \n",
        "\n",
        "<span style=\"color:red\"><strong>‚ö†Ô∏è WARNING:</strong> This is a MUST RUN step and the rest of this notebook will not work if you skip this step.</span>\n",
        "> **copy the following command and paste it in your terminal to launch your vLLM server:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```bash\n",
        "VLLM_USE_TRITON_FLASH_ATTN=0 \\\n",
        "vllm serve Qwen/Qwen3-30B-A3B \\\n",
        "    --served-model-name Qwen3-30B-A3B \\\n",
        "    --api-key abc-123 \\\n",
        "    --port 8000 \\\n",
        "    --enable-auto-tool-choice \\\n",
        "    --tool-call-parser hermes \\\n",
        "    --trust-remote-code\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upon successful launch, your server should be accepting incoming traffic through an OpenAI-compatible API. You must see a message such as the one below before moving on:\n",
        "\n",
        "<span style=\"color:red\">\n",
        "INFO: Started server process [210]<br>\n",
        "INFO: Waiting for application startup.<br>\n",
        "INFO: Application startup complete.\n",
        "</span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Now, return to this jupyter notebook and run the rest of this notebook here. Code cells in this notebook are executed by pressing `enter + shift` or the &#9658; button at the top bar. Let's start by creating a helper function to check server availbitly to use throughout this notebook.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile check_server.sh\n",
        "#!/bin/bash\n",
        "\n",
        "curl -s http://localhost:8000/v1/models -H \"Authorization: Bearer abc-123\" | grep '\"id\"' > /dev/null\n",
        "if [ $? -ne 0 ]; then\n",
        "  echo \"‚ùå Cannot reach the vLLM server at http://localhost:8000\"\n",
        "  echo \"‚û°Ô∏è  Please start the server as described in Step 1 before running this cell.\"\n",
        "  exit 1\n",
        "fi\n",
        "\n",
        "echo \"‚úÖ Server is reachable.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modify the file to turn it into an executable bash file and make it accessible globally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sudo mv check_server.sh /usr/local/bin/check_server\n",
        "!sudo chmod +x /usr/local/bin/check_server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's verify your server is running and it's reachable by executing the function we just defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!check_server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congratulations, you now just launched a powerful server that can serve any incoming requests and allow you to build amazing applications. Wasn't that easy?üéâ \n",
        "\n",
        "<a id=\"step2\"></a>\n",
        "\n",
        "## Step 2: Installing Dependencies\n",
        "\n",
        "<span style=\"color:red\"><strong>‚ö†Ô∏è WARNING:</strong> You must run the model in Step 1 successfully before moving to this step.</span>\n",
        "\n",
        "We are going to use `OpenManus`. Let's install the dependencies. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone https://github.com/FoundationAgents/OpenManus.git\n",
        "cd OpenManus\n",
        "\n",
        "# Remove conflicting system package\n",
        "echo \"Removing conflicting system package...\"\n",
        "apt remove --purge python3-blinker -y >/dev/null 2>&1\n",
        "echo \"Conflicting package removed.\"\n",
        "\n",
        "# Install key packages quietly without dependencies\n",
        "echo \"Installing key packages...\"\n",
        "pip install browsergym~=0.13.3 --no-deps -q >/dev/null 2>&1\n",
        "pip install browser-use~=0.1.40 -q >/dev/null 2>&1\n",
        "echo \"Key packages installed.\"\n",
        "\n",
        "# Patch requirements.txt to preserve huggingface-hub 0.31.4 \n",
        "echo \"Patching requirements.txt to avoid downgrading huggingface-hub...\"\n",
        "sed -i '/huggingface-hub/d' requirements.txt\n",
        "\n",
        "# Install rest of the requirements, preserving current package versions\n",
        "echo \"Installing remaining requirements...\"\n",
        "pip install -r requirements.txt --upgrade-strategy only-if-needed -q >/dev/null 2>&1\n",
        "echo \"Remaining requirements installed.\"\n",
        "\n",
        "# Install Playwright dependencies and browsers silently\n",
        "echo \"Installing Playwright dependencies and browsers...\"\n",
        "playwright install-deps >/dev/null 2>&1\n",
        "playwright install >/dev/null 2>&1\n",
        "echo \"Playwright dependencies and browsers installed.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change to our project directoy and let's start experimenting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Change to your desired directory\n",
        "os.chdir('OpenManus')\n",
        "\n",
        "# Verify you're in the new directory\n",
        "print(\"Current directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<a id=\"step3\"></a>\n",
        "\n",
        "## Step 3: Create a simple instance of OpenManus Agent\n",
        "\n",
        "Let's start by creating a config file and connect OpenAI Compatible endpoint. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_path = \"./config/config.toml\"\n",
        "\n",
        "config_content = \"\"\"\n",
        "[llm]\n",
        "model = \"Qwen3-30B-A3B\"\n",
        "base_url = \"http://localhost:8000/v1\"\n",
        "api_key = \"abc-123\"\n",
        "max_tokens = 4096\n",
        "temperature = 0.0\n",
        "\n",
        "[browser]\n",
        "headless = true\n",
        "\n",
        "[mcp]\n",
        "server_reference = \"app.mcp.server\"\n",
        "\n",
        "[runflow]\n",
        "use_data_analysis_agent = false\n",
        "\"\"\"\n",
        "\n",
        "# Write the cleaned config\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(config_content.strip() + \"\\n\")\n",
        "\n",
        "print(f\"Wrote cleaned config to: {config_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test the OpenManus agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!check_server && python main.py --prompt \"I am in San Jose, I want to make a Mapo Tofu. Tell me where to get all the ingredients I need.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's see how we can get some real-time data from our agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!check_server && python main.py --prompt \"what is the date today\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That is great, OpenManus attempts using the tools it has at its disposal to get this information. What if you wanted to use ready-to-use MCP server? Let's see how we can do this in the next section. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "<a id=\"step4\"></a>\n",
        "\n",
        "## Step 4: Adding a MCP server\n",
        "\n",
        "Now that we have learned how to create a custom tool and provide the agent access to this tool, let's explore a trendy topic of [Model Context Protocol](https://modelcontextprotocol.io/introduction). We are going to explore how we can replace our custom tool with a simple MCP server that can serve our agent and provide similar information.\n",
        "\n",
        "**Why MCP?** MCP servers provide:\n",
        "- ‚úÖ Standardized API interfaces\n",
        "- üîÑ Reusable across projects\n",
        "- üì¶ Pre-built functionality\n",
        "\n",
        "Let's replace our custom time tool with an official MCP time server:\n",
        "\n",
        "### Installing Time MCP Server\n",
        "\n",
        "Start by installing this MCP server:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q mcp-server-time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, create our MCP config file. Start by creating the variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mcp_info = {\n",
        "  \"mcpServers\": {\n",
        "    \"time\": {\n",
        "      \"type\": \"stdio\",\n",
        "      \"command\": \"python\",\n",
        "      \"args\": [\"-m\", \"mcp_server_time\", \"--local-timezone=America/New_York\"]\n",
        "    }\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then write it as `mcp.json` under `config` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## create the mcp.json under OpenManus/config/mcp.json\n",
        "import json\n",
        "\n",
        "# Write mcp.json file\n",
        "with open(\"config/mcp.json\", \"w\") as f:\n",
        "    json.dump(mcp_info, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "Great, let's see if the agent can use the MCP to give us the correct time now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!check_server && python main.py --prompt \"Tell me the time in San Francisco\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Tadaa! Now you have officially used an MCP server to power-up your agent. In the next section, we demonstrate how you can your turn many ideas into real working projects by using 100s of free or paid MCP servers available today.\n",
        "\n",
        "\n",
        "\n",
        "<a id=\"step6\"></a>\n",
        "\n",
        "## Step 5: Turn your agent to Multi-MCP user\n",
        "\n",
        "As we experienced in the last section, MCP servers are easy to use and they provide a standard way of providing LLMs the tools we need. There are already thousands of MCP servers available for us to use. There are some MCP trackers that you can utilize to find out about available servers. Here are some for your reference:\n",
        "- https://github.com/modelcontextprotocol/servers\n",
        "- https://mcp.so/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "In this section of the workshop, we are going to build an agent that can help you browse available Airbnbs to book. We can now build on top of what we have so far and add an open-source Airbnb MCP server to our agent. To do so, let's start by defining our Airbnb server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Node.js 18 via NodeSource\n",
        "!curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\n",
        "!apt install -y nodejs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's verify our installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!node -v && npm -v && npx --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's update our agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mcp_info = {\n",
        "    \"mcpServers\": {\n",
        "        \"time\": {\n",
        "            \"type\": \"stdio\",\n",
        "            \"command\": \"python\",\n",
        "            \"args\": [\n",
        "                \"-m\",\n",
        "                \"mcp_server_time\",\n",
        "                \"--local-timezone=America/New_York\"\n",
        "            ]\n",
        "        },\n",
        "        \"airbnb\": {\n",
        "            \"type\": \"stdio\", \n",
        "            \"command\": \"npx\",\n",
        "            \"args\": [\n",
        "                \"-y\",\n",
        "                \"@openbnb/mcp-server-airbnb\",\n",
        "                \"--ignore-robots-txt\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to write our updated config file under `config` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## create the mcp.json under OpenManus/config/mcp.json\n",
        "import json\n",
        "# Write mcp.json file\n",
        "with open(\"config/mcp.json\", \"w\") as f:\n",
        "    json.dump(mcp_info, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's try our agent and see if it can browse through Airbnb listings. Time to test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!check_server && python main.py --prompt \"Find a place to stay in Vancouver for next Sunday for 3 nights for 2 adults?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "<a id=\"step7\"></a>\n",
        "\n",
        "## Step 7: Challenge - Expand the Agent\n",
        "\n",
        "**Task:** Add weather integration using an appropiate MCP server:\n",
        "1. Launch weather MCP server\n",
        "2. Add to agent's tools\n",
        "3. Make agent suggest best travel dates based on weather\n",
        "\n",
        "**Judging Criteria:**\n",
        "‚úÖ Functional weather integration\n",
        "üéØ Logical tool selection\n",
        "üí° Creative use of multiple tools\n",
        "\n",
        "<span style=\"color:red\"><strong>‚ö†Ô∏è FINAL STEP:</strong> When you're done, make sure to stop the model and free GPU resources by pressing <code>Ctrl+C</code> in the terminal where the server is running.</span>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
