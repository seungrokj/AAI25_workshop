{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Microsoft Build May 19, 2025: Build Your AI Agent with MCPs using vLLM, Pydantic AI, and AMD MI300X GPU\n",
    "\n",
    "Welcome to this hands-on workshop! Throughout this tutorial, we'll leverage AMD GPUs and **Model Context Protocol (MCP)** ,an open standard for exposing LLM tools via API, to deploy powerful language models like Qwen3. Key components:\n",
    "- üñ•Ô∏è **vLLM** for GPU-optimized inference\n",
    "- üõ†Ô∏è **Pydantic-AI** for agent/tool management\n",
    "- üîå **MCP Servers** for pre-built tool integration\n",
    "\n",
    "You'll learn how to set up your environment, deploy large language models like Qwen3, connect them to real-world tools using MCP, and build a conversational agent capable of reasoning and taking actions.\n",
    "\n",
    "By the end of this workshop, you‚Äôll have built an AI-powered Airbnb assistant agent‚Äîone that can find a place to stay based on your preferences like location, budget, and travel dates.\n",
    "\n",
    "Let‚Äôs dive in!\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Step 1: Launching vLLM Server on AMD GPUs](#step1)\n",
    "- [Step 2: Installing Dependencies](#step2)\n",
    "- [Step 3: Create a simple instance of Pydantic-AI Agent](#step3)\n",
    "- [Step 4: Write a Date/Time Tool for Your Agent](#step4)\n",
    "- [Step 5: Replace Your Date/time Tool with a MCP server](#step5)\n",
    "- [Step 6: Turn your agent to an Airbnb finder](#step6)\n",
    "- [Step 7: Challenge with Prize](#step7)\n",
    "\n",
    "<a id=\"step1\"></a>\n",
    "\n",
    "## Step 1: Launch a vLLM Server\n",
    "\n",
    "In this workshop we are going to use [vLLM](https://github.com/vllm-project/vllm) as our inference serving engine. vLLM provides many benefits such as fast model execution, extensive list of supported models, easy to use, and best of all it's open-source. \n",
    "\n",
    "### Deploy Qwen3-30B-A3B Model with vLLM\n",
    "\n",
    "In this workshop we have already configured `VLLM_PORT` for you. Let's verify the port number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!echo $VLLM_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Time to start your vLLM server and creating an end-point for your LLM. Let's open a terminal using your Jupyter server. Then run the following command in this terminal to start the vLLM server:\n",
    "\n",
    "```bash\n",
    "VLLM_USE_TRITON_FLASH_ATTN=0 \\\n",
    "vllm serve Qwen/Qwen3-30B-A3B \\\n",
    "    --served-model-name Qwen3-30B-A3B \\\n",
    "    --api-key abc-123 \\\n",
    "    --port $VLLM_PORT \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser hermes \\\n",
    "    --trust-remote-code\n",
    "```\n",
    "\n",
    "Open another terminal and monitor the GPU utilization by running this command:\n",
    "\n",
    "```bash\n",
    "watch rocm-smi\n",
    "```\n",
    "\n",
    "Upon successful launch, your server should be accepting incoming traffic through an OpenAI-compatible API. Let's set some environment variables for our server so we can use throughout this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_URL = f\"http://localhost:{os.environ['VLLM_PORT']}/v1\"\n",
    "\n",
    "os.environ[\"BASE_URL\"]    = BASE_URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"abc-123\"   \n",
    "\n",
    "print(\"Config set:\", BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify your model is available at the `BASE_URL` we just set by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!curl http://localhost:$VLLM_PORT/v1/models -H \"Authorization: Bearer $OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you now just launched a powerful server that can serve any incoming request and allowing you to build amazing applications. Wasn't that easy?üéâ \n",
    "\n",
    "<a id=\"step2\"></a>\n",
    "\n",
    "## Step 2: Installing Dependencies\n",
    "\n",
    "We are going to use `Pydantic AI`. Let's install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q pydantic_ai openai     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"step3\"></a>\n",
    "\n",
    "## Step 3: Create a simple instance of Pydantic-AI Agent\n",
    "\n",
    "Let's start by creating a custom OpenAI Compatible endpoint for our agent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "\n",
    "provider = OpenAIProvider(\n",
    "    base_url=os.environ[\"BASE_URL\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "agent_model = OpenAIModel(\"Qwen3-30B-A3B\", provider=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating an instance the `Agent` class from `pydantic_ai`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    model=agent_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to test the agent. `pydantic_ai` provides multiple ways to run `Agent`. You can learn more about it [here](https://ai.pydantic.dev/agents/#running-agents).\n",
    "\n",
    "In this workshop, we are running in `async` mode. We are going to define a helper function that allows us to quickly test our agent throughout this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pydantic_ai.mcp import MCPServerStdio\n",
    "async def run_async(prompt: str) -> str:\n",
    "    async with agent.run_mcp_servers():\n",
    "        result = await agent.run(prompt)\n",
    "        return result.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the agent by calling this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "await run_async(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! now that we have the basics of creating an agent instance, and connecting it to the model we started serving with vLLM earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "\n",
    "## Step 4: Write a Date/Time Tool for Your Agent\n",
    "\n",
    "LLMs naturally rely on their training data to respond to your prompts. Therefore, the agent we just defined fails to answer a factual question that falls outside of it's training knowledge. Let's show this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "await run_async(\"What‚Äôs the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is no surprise that the model failed to answer this question. Now, it's time to power-up your LLM by providing `agent` a function that can get the current date. The process of an LLM triggering a function call is commonly referred to as `Tool Calling` or `Function Calling`. In this workshop we are going to take advantage of `pydantic-ai`'s agent `tools` to provide our agent appropriate tools. First, we need to define a custom tool. Below is how we can define a tool in this framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pydantic_ai import Tool          \n",
    "@Tool\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Return the current date/time as an ISO-formatted string.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to provide this tool to our Agent, as this will notify the LLM about the existence of such a tool we just definied. This is simply done by just providing the function signiture of the tool we just defined to our agent constructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model=agent_model,\n",
    "    tools=[get_current_date],\n",
    "    system_prompt = (\n",
    "        \"You have access to:\\n\"\n",
    "        \"   1. get_current_time(params: dict)\\n\"\n",
    "        \"Use this tool for date/time questions.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "await run_async(\"What‚Äôs the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done on building an agent with access to real-time data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"step5\"></a>\n",
    "\n",
    "## Step 5: Replace Your Date/time Tool with a MCP server\n",
    "\n",
    "Now that we learned how to create a custom tool and provide the agent access to this tool. Let's now explore a trendy topic of [Model Context Protocol](https://modelcontextprotocol.io/introduction). We are going to explore how we can replace our custom tool with a simple MCP server that can serve our agent and provide similar information.\n",
    "\n",
    "**Why MCP?** MCP servers provide:\n",
    "- ‚úÖ Standardized API interfaces\n",
    "- üîÑ Reusable across projects\n",
    "- üì¶ Pre-built functionality\n",
    "\n",
    "Let's replace our custom time tool with an official MCP time server:\n",
    "\n",
    "### Installing Time MCP Server\n",
    "\n",
    "We are going to start by installing this MCP server:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q mcp-server-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our time_server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic_ai.mcp import MCPServerStdio\n",
    "\n",
    "time_server = MCPServerStdio(\n",
    "    \"python\",\n",
    "    args=[\n",
    "        \"-m\", \"mcp_server_time\",\n",
    "        \"--local-timezone=America/New_York\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's modify our agent to remove our previously defined tool, and add this MCP server instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model=agent_model,\n",
    "    mcp_servers=[time_server],\n",
    "    system_prompt = (\n",
    "        \"You are a helpful agent and you have access to this tool:\\n\"\n",
    "        \"   get_current_time(params: dict)\\n\"\n",
    "        \"When the user asks for the current date or time, call get_current_time.\\n\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Great, let's see if the agent can use the MCP to give us the correct time now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "await run_async(\"What‚Äôs the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tadaa! Now you have officially used an MCP server to power-up your agent. In the next section we show how you can your turn many ideas into real working projects by using 100s of free or paid MCP servers available today.\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"step6\"></a>\n",
    "\n",
    "## Step 6: Turn your agent to an airbnb finder\n",
    "\n",
    "As we experience in the last section, MCP servers are really easy to use and they provide a standard way of providing LLMs the tools we need. There are already thousands of MCP servers available for us to use. There are some MCP trackers that you can always use to find out about available servers. Here are some for your reference:\n",
    "- https://github.com/modelcontextprotocol/servers\n",
    "- https://mcp.so/\n",
    "\n",
    "We are going to use npx to launch out next server. Therefore, let's install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install Node.js 18 via NodeSource\n",
    "!curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\n",
    "!apt install -y nodejs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify `npm` and `npx` installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!node -v && npm -v && npx --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this part of the workshop we are going to build an agent that can help you browse available Airbnbs to book. We can now build on top of what we have so far and add an open-source Airbnb MCP server to our agent. To do so, let's start by defining our Airbnb server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "airbnb_server = MCPServerStdio(\n",
    "    \"npx\", args=[\"-y\", \"@openbnb/mcp-server-airbnb\", \"--ignore-robots-txt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You have access to three tools:\n",
    "1. get_current_time(params: dict)\n",
    "2. airbnb_search(params: dict)\n",
    "3. airbnb_listing_details(params: dict)\n",
    "When the user asks for listings, first call get_current_time, then airbnb_search, etc.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    model=agent_model,\n",
    "    mcp_servers=[time_server, airbnb_server],\n",
    "    system_prompt=system_prompt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try our agent and see if it can browse through Airbnb listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "await run_async(\"Find a place to stay in Vancouver for next Sunday for 3 nights for 2 adults?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"step7\"></a>\n",
    "\n",
    "## Step 7: Challenge - Expand the Agent\n",
    "\n",
    "**Task:** Add weather integration using an appropiate MCP server:\n",
    "1. Launch weather MCP server\n",
    "2. Add to agent's tools\n",
    "3. Make agent suggest best travel dates based on weather\n",
    "\n",
    "**Judging Criteria:**\n",
    "‚úÖ Functional weather integration\n",
    "üéØ Logical tool selection\n",
    "üí° Creative use of multiple tools\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
