{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Developing AI Agents with AMD GPUs: Build Your OpenManus Agent with MCPs using vLLM, and AMD MI300X GPU\n",
    "\n",
    "Welcome to this hands-on workshop! Throughout this tutorial, we'll leverage AMD GPUs and **Model Context Protocol (MCP)** ,an open standard for exposing LLM tools via API, to deploy powerful language models like Qwen3. Key components:\n",
    "- üñ•Ô∏è **vLLM** for GPU-optimized inference\n",
    "- üõ†Ô∏è **OpenManus** for agent/tool management\n",
    "- üîå **MCP Servers** for pre-built tool integration\n",
    "\n",
    "You'll learn how to set up your environment, deploy large language models like Qwen3, connect them to real-world tools using MCP, and build a conversational agent capable of reasoning and taking actions.\n",
    "\n",
    "By the end of this workshop, you‚Äôll have built an AI-powered Airbnb assistant agent‚Äîone that can find a place to stay based on your preferences like location, budget, and travel dates.\n",
    "\n",
    "Let‚Äôs dive in!\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Step 1: Launching vLLM Server on AMD GPUs](#step1)\n",
    "- [Step 2: Installing Dependencies](#step2)\n",
    "- [Step 3: Create a simple instance of OpenManus](#step3)\n",
    "- [Step 4: Tsing MCP server for OpenManus Agent](#step4)\n",
    "- [Step 5: Challenge with Prize](#step5)\n",
    "\n",
    "<a id=\"step1\"></a>\n",
    "\n",
    "## Step 1: Launch a vLLM Server\n",
    "\n",
    "In this workshop we are going to use [vLLM](https://github.com/vllm-project/vllm) as our inference serving engine. vLLM provides many benefits such as fast model execution, extensive list of supported models, easy to use, and best of all it's open-source. \n",
    "\n",
    "### Deploy Qwen3-30B-A3B Model with vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Time to start your vLLM server and creating an end-point for your LLM. Let's open a terminal using your Jupyter server. Then run the following command in this terminal to start the vLLM server:\n",
    "\n",
    "```bash\n",
    "VLLM_USE_TRITON_FLASH_ATTN=0 \\\n",
    "vllm serve Qwen/Qwen3-30B-A3B \\\n",
    "    --served-model-name Qwen3-30B-A3B \\\n",
    "    --api-key abc-123 \\\n",
    "    --port 8000 \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser hermes \\\n",
    "    --trust-remote-code\n",
    "```\n",
    "\n",
    "Open another terminal and monitor the GPU utilization by running this command:\n",
    "\n",
    "```bash\n",
    "watch rocm-smi\n",
    "```\n",
    "\n",
    "Upon successful launch, your server should be accepting incoming traffic through an OpenAI-compatible API. Let's set some environment variables for our server so we can use throughout this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_URL = f\"http://localhost:8000/v1\"\n",
    "\n",
    "os.environ[\"BASE_URL\"]    = BASE_URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"abc-123\"   \n",
    "\n",
    "print(\"Config set:\", BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify your model is available at the `BASE_URL` we just set by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8000/v1/models -H \"Authorization: Bearer $OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you now just launched a powerful server that can serve any incoming request and allowing you to build amazing applications. Wasn't that easy?üéâ \n",
    "\n",
    "<a id=\"step2\"></a>\n",
    "\n",
    "## Step 2: Installing Dependencies\n",
    "\n",
    "We are going to use `OpenManus`. Let's install the dependencies. Note, ideally in your :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/FoundationAgents/OpenManus.git\n",
    "cd OpenManus\n",
    "apt remove --purge python3-blinker -y\n",
    "pip install browsergym~=0.13.3 --no-deps\n",
    "pip install browser-use~=0.1.40\n",
    "pip install -r requirements.txt \n",
    "playwright install-deps\n",
    "playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change to our project directoy and let's start experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change to your desired directory\n",
    "os.chdir('OpenManus')\n",
    "\n",
    "# Verify you're in the new directory\n",
    "print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"step3\"></a>\n",
    "\n",
    "## Step 3: Create a simple instance of OpenManus Agent\n",
    "\n",
    "Let's start by creating a config file and connect OpenAI Compatible endpoint. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./config/config.toml\"\n",
    "\n",
    "config_content = \"\"\"\n",
    "[llm]\n",
    "model = \"Qwen3-30B-A3B\"\n",
    "base_url = \"http://localhost:8000/v1\"\n",
    "api_key = \"abc-123\"\n",
    "max_tokens = 4096\n",
    "temperature = 0.0\n",
    "\n",
    "[browser]\n",
    "headless = true\n",
    "\n",
    "[mcp]\n",
    "server_reference = \"app.mcp.server\"\n",
    "\n",
    "[runflow]\n",
    "use_data_analysis_agent = false\n",
    "\"\"\"\n",
    "\n",
    "# Write the cleaned config\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write(config_content.strip() + \"\\n\")\n",
    "\n",
    "print(f\"Wrote cleaned config to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the OpenManus agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --prompt \"I am in San Jose, I want to make a Mapo Tofu. Tell me where to get all the ingredients I need.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see how we can get some realtime data from our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --prompt \"what is the date today\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is great, OpenManus attempts using the tools it has at its disposal to get this information. What if you wanted to use ready to use MCP server? Let's see how we can do this in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"step4\"></a>\n",
    "\n",
    "## Step 4: Adding a MCP server\n",
    "\n",
    "Now that we learned how to create a custom tool and provide the agent access to this tool. Let's now explore a trendy topic of [Model Context Protocol](https://modelcontextprotocol.io/introduction). We are going to explore how we can replace our custom tool with a simple MCP server that can serve our agent and provide similar information.\n",
    "\n",
    "**Why MCP?** MCP servers provide:\n",
    "- ‚úÖ Standardized API interfaces\n",
    "- üîÑ Reusable across projects\n",
    "- üì¶ Pre-built functionality\n",
    "\n",
    "Let's replace our custom time tool with an official MCP time server:\n",
    "\n",
    "### Installing Time MCP Server\n",
    "\n",
    "We are going to start by installing this MCP server:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mcp-server-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our MCP config file. Start by creating the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_info = {\n",
    "  \"mcpServers\": {\n",
    "    \"time\": {\n",
    "      \"type\": \"stdio\",\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [\"-m\", \"mcp_server_time\", \"--local-timezone=America/New_York\"]\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then write it as `mcp.json` under `config` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the mcp.json under OpenManus/config/mcp.json\n",
    "import json\n",
    "\n",
    "# Write mcp.json file\n",
    "with open(\"config/mcp.json\", \"w\") as f:\n",
    "    json.dump(mcp_info, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Great, let's see if the agent can use the MCP to give us the correct time now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --prompt \"Tell me the time in San Francisco\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tadaa! Now you have officially used an MCP server to power-up your agent. In the next section we show how you can your turn many ideas into real working projects by using 100s of free or paid MCP servers available today.\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"step6\"></a>\n",
    "\n",
    "## Step 5: Turn your agent to Multi-MCP user\n",
    "\n",
    "As we experience in the last section, MCP servers are really easy to use and they provide a standard way of providing LLMs the tools we need. There are already thousands of MCP servers available for us to use. There are some MCP trackers that you can always use to find out about available servers. Here are some for your reference:\n",
    "- https://github.com/modelcontextprotocol/servers\n",
    "- https://mcp.so/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this part of the workshop we are going to build an agent that can help you browse available Airbnbs to book. We can now build on top of what we have so far and add an open-source Airbnb MCP server to our agent. To do so, let's start by defining our Airbnb server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Node.js 18 via NodeSource\n",
    "!curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\n",
    "!apt install -y nodejs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!node -v && npm -v && npx --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_info = {\n",
    "    \"mcpServers\": {\n",
    "        \"time\": {\n",
    "            \"type\": \"stdio\",\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\n",
    "                \"-m\",\n",
    "                \"mcp_server_time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        },\n",
    "        \"airbnb\": {\n",
    "            \"type\": \"stdio\", \n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@openbnb/mcp-server-airbnb\",\n",
    "                \"--ignore-robots-txt\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write our updsated config file under `config` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the mcp.json under OpenManus/config/mcp.json\n",
    "import json\n",
    "# Write mcp.json file\n",
    "with open(\"config/mcp.json\", \"w\") as f:\n",
    "    json.dump(mcp_info, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try our agent and see if it can browse through Airbnb listings.Time to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --prompt \"Find a place to stay in Vancouver for next Sunday for 3 nights for 2 adults?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"step7\"></a>\n",
    "\n",
    "## Step 7: Challenge - Expand the Agent\n",
    "\n",
    "**Task:** Add weather integration using an appropiate MCP server:\n",
    "1. Launch weather MCP server\n",
    "2. Add to agent's tools\n",
    "3. Make agent suggest best travel dates based on weather\n",
    "\n",
    "**Judging Criteria:**\n",
    "‚úÖ Functional weather integration\n",
    "üéØ Logical tool selection\n",
    "üí° Creative use of multiple tools\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
